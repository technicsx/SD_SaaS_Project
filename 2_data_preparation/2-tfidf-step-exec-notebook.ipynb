{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "a78848a5d0fd4e34a5897a47dc230dcc",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 1
    },
    "deepnote_cell_type": "text-cell-h1",
    "formattedRanges": [
     {
      "fromCodePoint": 0,
      "marks": {
       "bold": true
      },
      "toCodePoint": 17,
      "type": "marks"
     }
    ],
    "is_collapsed": false
   },
   "source": [
    "# ISW preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "6f16cd0deaeb4efd836f56e3018ad03d",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 7
    },
    "deepnote_cell_type": "text-cell-h3",
    "formattedRanges": [],
    "is_collapsed": false
   },
   "source": [
    "### Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "cell_id": "9b1434b44565402a970783f71fd427f1",
    "deepnote_app_coordinates": {
     "h": 17,
     "w": 12,
     "x": 0,
     "y": 13
    },
    "deepnote_app_is_output_hidden": true,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 7289,
    "execution_start": 1680019062987,
    "source_hash": "57d56a81"
   },
   "outputs": [],
   "source": [
    "# to be uncommented if dependencies are not installed\n",
    "\n",
    "# %pip install nltk num2words scikit-learn pandas numpy python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "7133bbff0c7644cdac4b3394e5327dd8",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 31
    },
    "deepnote_cell_type": "text-cell-h3",
    "formattedRanges": [],
    "is_collapsed": false
   },
   "source": [
    "### Import and download all dependecies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "cell_id": "8e138bae7b244bdea0d767dd645b62d8",
    "deepnote_app_coordinates": {
     "h": 25,
     "w": 12,
     "x": 0,
     "y": 37
    },
    "deepnote_app_is_output_hidden": true,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 2920,
    "execution_start": 1680019070283,
    "source_hash": "ec535853"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\lap2r\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\lap2r\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\lap2r\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\lap2r\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "import pickle\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "from num2words import num2words\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "import pandas as pd\n",
    "import string\n",
    "from zipfile import ZipFile, ZipInfo\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "3529039ed0544e6f984776537deef311",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 63
    },
    "deepnote_cell_type": "text-cell-h3",
    "formattedRanges": [],
    "is_collapsed": false
   },
   "source": [
    "### Define preprocessing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "cell_id": "86fdbb97c0564339842d296aedb07a7d",
    "deepnote_app_coordinates": {
     "h": 47,
     "w": 12,
     "x": 0,
     "y": 69
    },
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 5,
    "execution_start": 1680019073219,
    "source_hash": "bfe72237"
   },
   "outputs": [],
   "source": [
    "# Functions\n",
    "def to_lower_case(text):\n",
    "  return \"\".join([i.lower() for i in text])\n",
    "\n",
    "stop_punctuation = string.punctuation\n",
    "def remove_punctuation(text):\n",
    "  return \"\".join([i for i in text if i not in stop_punctuation])\n",
    "\n",
    "def remove_long_dash(text):\n",
    "  return re.sub(r'â€”', ' ', text)\n",
    "\n",
    "def remove_urls(text):\n",
    "  return re.sub(r'http\\S+', '', text)\n",
    "\n",
    "def remove_one_letter_words(tokens):\n",
    "  return list(filter(lambda token: len(token) > 1, tokens))\n",
    "\n",
    "def tokenize_text(text):\n",
    "  return nltk.tokenize.word_tokenize(text)\n",
    "\n",
    "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "frequent_words = {'russian', 'force', 'forces', 'ukrainian', 'ukraine', 'oblast' 'ukraine', 'military', 'reported', 'effort', 'likely',\n",
    "                  'claimed', 'russia', 'area', 'operation', 'continued', 'city', 'general', 'near', 'attack',\n",
    "                  'official', 'staff', 'also', 'stated', 'source', 'oblast', 'pm', 'am'}\n",
    "month_names = {'january', 'february', 'march', 'april', 'may', 'june', 'july', 'august', 'september', 'october', 'november', 'december'}\n",
    "# avoid_stop_words = {\"not\", \"n't\", \"no\"}\n",
    "# stop_words = stop_words - avoid_stop_words\n",
    "stop_words = stop_words.union(frequent_words)\n",
    "stop_words = stop_words.union(month_names)\n",
    "\n",
    "def remove_stop_words(tokens):\n",
    "  return [i for i in tokens if i not in stop_words]\n",
    "\n",
    "def do_stemming(tokens):\n",
    "  ps = nltk.PorterStemmer()\n",
    "  return [ps.stem(word) for word in tokens]\n",
    "\n",
    "def do_lemmatization(tokens):\n",
    "  wn = nltk.WordNetLemmatizer()\n",
    "  return [wn.lemmatize(word) for word in tokens]\n",
    "\n",
    "def remove_numeric_words(text):\n",
    "  return re.sub(r'\\S*\\d+\\S*', '', text)\n",
    "\n",
    "def convert_nums_to_words(data):\n",
    "  tokens = data\n",
    "  new_text = []\n",
    "  for word in tokens:\n",
    "    if word.isdigit():\n",
    "      if int(word)<1000000000:\n",
    "        word = num2words(word)\n",
    "      else: \n",
    "        word = \"\"\n",
    "    new_text.extend(tokenize_text(re.sub(\"(-|,\\s?)|\\s+\", \" \", word)))\n",
    "  return new_text\n",
    "\n",
    "def do_preprocessing(data):\n",
    "  text_clean = data\n",
    "  text_clean = remove_urls(text_clean)\n",
    "  text_clean = remove_punctuation(text_clean)\n",
    "  text_clean = remove_long_dash(text_clean)\n",
    "  text_clean = to_lower_case(text_clean)\n",
    "  text_clean = remove_numeric_words(text_clean)\n",
    "  words = tokenize_text(text_clean)\n",
    "  words = remove_one_letter_words(words)\n",
    "  words = remove_stop_words(words)\n",
    "  lemmatized = do_lemmatization(words)\n",
    "  res = convert_nums_to_words(lemmatized)\n",
    "  return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "3adde034474b4e01b133850f0598de42",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 117
    },
    "deepnote_cell_type": "text-cell-h3",
    "formattedRanges": [],
    "is_collapsed": false
   },
   "source": [
    "### Zip opening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "cell_id": "9b0dd02d4a1d46c9910d60f25e24d79f",
    "deepnote_app_coordinates": {
     "h": 13,
     "w": 12,
     "x": 0,
     "y": 123
    },
    "deepnote_app_is_output_hidden": false,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 928,
    "execution_start": 1680019073228,
    "source_hash": "2e7a14d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading folder contents\n",
      "Successfully read the input data\n"
     ]
    }
   ],
   "source": [
    "# use env config\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(\"../.env\")\n",
    "\n",
    "# specifying the input folder\n",
    "folder_name = \"../\" + os.getenv(\"ISW_SCRAPPING_FODLER\")\n",
    "\n",
    "df = pd.DataFrame(columns=[\"Name\", \"Date\", \"Text\"])\n",
    "\n",
    "df_list = []\n",
    "\n",
    "print(\"Reading folder contents\")\n",
    "for root, dirs, files in os.walk(folder_name):\n",
    "    for filename in files:\n",
    "        if filename.endswith('.txt'):\n",
    "            with open(os.path.join(root, filename), encoding='utf-8') as file:\n",
    "                name = filename.split('.')[0]\n",
    "                date = filename.replace(\"assessment-\", \"\")\n",
    "                text = file.read()\n",
    "                row_df = pd.DataFrame({\"Name\": [name], \"Date\": [date], \"Text\": [text]})\n",
    "                df_list.append(row_df)\n",
    "df = pd.concat(df_list, ignore_index=True)\n",
    "print(\"Successfully read the input data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "17b1138cf7c9457aa477a30ccadacbf0",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 137
    },
    "deepnote_cell_type": "text-cell-h3",
    "formattedRanges": [],
    "is_collapsed": false
   },
   "source": [
    "### TF-IDF creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "cell_id": "09538d60d2d54f25bc752ee441583c16",
    "deepnote_app_coordinates": {
     "h": 44,
     "w": 12,
     "x": 0,
     "y": 143
    },
    "deepnote_cell_type": "code",
    "deepnote_table_loading": false,
    "deepnote_table_state": {
     "filters": [],
     "pageIndex": 0,
     "pageSize": 10,
     "sortBy": []
    },
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 95017,
    "execution_start": 1680019074182,
    "source_hash": "33982a8a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Find tokens\n",
      "Create vectors\n",
      "Into result\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Date</th>\n",
       "      <th>Keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>assessment-2022-02-24</td>\n",
       "      <td>2022-02-24.txt</td>\n",
       "      <td>{'airport': 0.2989225615551494, 'kyiv': 0.2142...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>assessment-2022-02-25</td>\n",
       "      <td>2022-02-25.txt</td>\n",
       "      <td>{'kyiv': 0.3623260669568973, 'local': 0.189735...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>assessment-2022-02-26</td>\n",
       "      <td>2022-02-26.txt</td>\n",
       "      <td>{'kyiv': 0.4540653793502749, 'zaprozhia': 0.15...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>assessment-2022-02-28</td>\n",
       "      <td>2022-02-28.txt</td>\n",
       "      <td>{'kyiv': 0.3075720489357905, 'asset': 0.176497...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>assessment-2022-03-01</td>\n",
       "      <td>2022-03-01.txt</td>\n",
       "      <td>{'kyiv': 0.374212983451302, 'chernihiv': 0.233...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394</th>\n",
       "      <td>assessment-2023-03-29</td>\n",
       "      <td>2023-03-29.txt</td>\n",
       "      <td>{'moskalev': 0.20967599808875628, 'wagner': 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>assessment-2023-03-30</td>\n",
       "      <td>2023-03-30.txt</td>\n",
       "      <td>{'csto': 0.23584684478345108, 'khodakovsky': 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>assessment-2023-03-31</td>\n",
       "      <td>2023-03-31.txt</td>\n",
       "      <td>{'lukashenko': 0.2557744590950341, 'antiwester...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>assessment-2023-04-01</td>\n",
       "      <td>2023-04-01.txt</td>\n",
       "      <td>{'conscription': 0.22151586733175985, 'offensi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>assessment-2023-04-02</td>\n",
       "      <td>2023-04-02.txt</td>\n",
       "      <td>{'fomin': 0.7665690669907448, 'assassination':...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>399 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Name            Date  \\\n",
       "0    assessment-2022-02-24  2022-02-24.txt   \n",
       "1    assessment-2022-02-25  2022-02-25.txt   \n",
       "2    assessment-2022-02-26  2022-02-26.txt   \n",
       "3    assessment-2022-02-28  2022-02-28.txt   \n",
       "4    assessment-2022-03-01  2022-03-01.txt   \n",
       "..                     ...             ...   \n",
       "394  assessment-2023-03-29  2023-03-29.txt   \n",
       "395  assessment-2023-03-30  2023-03-30.txt   \n",
       "396  assessment-2023-03-31  2023-03-31.txt   \n",
       "397  assessment-2023-04-01  2023-04-01.txt   \n",
       "398  assessment-2023-04-02  2023-04-02.txt   \n",
       "\n",
       "                                              Keywords  \n",
       "0    {'airport': 0.2989225615551494, 'kyiv': 0.2142...  \n",
       "1    {'kyiv': 0.3623260669568973, 'local': 0.189735...  \n",
       "2    {'kyiv': 0.4540653793502749, 'zaprozhia': 0.15...  \n",
       "3    {'kyiv': 0.3075720489357905, 'asset': 0.176497...  \n",
       "4    {'kyiv': 0.374212983451302, 'chernihiv': 0.233...  \n",
       "..                                                 ...  \n",
       "394  {'moskalev': 0.20967599808875628, 'wagner': 0....  \n",
       "395  {'csto': 0.23584684478345108, 'khodakovsky': 0...  \n",
       "396  {'lukashenko': 0.2557744590950341, 'antiwester...  \n",
       "397  {'conscription': 0.22151586733175985, 'offensi...  \n",
       "398  {'fomin': 0.7665690669907448, 'assassination':...  \n",
       "\n",
       "[399 rows x 3 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Find tokens\")          \n",
    "df[\"Tokens\"] = df[\"Text\"].apply(lambda d: \" \".join(do_preprocessing(d)))\n",
    "\n",
    "# To be uncommented if you want to see the most common words\n",
    "#\n",
    "# print(\"Find most common words\")\n",
    "#\n",
    "# all_words = []\n",
    "# for tokens in df[\"Tokens\"]:\n",
    "#   for word in tokens.split(\" \"):\n",
    "#     all_words.append(word)\n",
    "# all_words = nltk.FreqDist(all_words)\n",
    "# print(\"Top 30 frequenty used words: \")\n",
    "# print(all_words.most_common(30))\n",
    "\n",
    "\n",
    "filenames = df[\"Name\"]\n",
    "dates = df[\"Date\"]\n",
    "\n",
    "print(\"Create vectors\")\n",
    "tfidf = TfidfVectorizer(smooth_idf=True,use_idf=True)\n",
    "vectors = tfidf.fit_transform(df[\"Tokens\"])\n",
    "\n",
    "# store content\n",
    "with open(\"results/tfidf.pkl\", \"wb\") as handle:\n",
    "  pickle.dump(tfidf, handle)\n",
    "\n",
    "feature_names = tfidf.get_feature_names_out()\n",
    "dense = vectors.todense()\n",
    "denselist = dense.tolist()\n",
    "df = pd.DataFrame(denselist, columns=feature_names)\n",
    "dictionaries = df.to_dict(orient='records')\n",
    "\n",
    "print(\"Into result\")\n",
    "res = __builtins__.zip(filenames, dates, dictionaries)\n",
    "res_df = pd.DataFrame(res, columns=[\"Name\",\"Date\",\"Keywords\"])\n",
    "res_df[\"Keywords\"] = res_df[\"Keywords\"].apply(lambda d: {k: v for k, v in d.items() if v > 0})\n",
    "res_df[\"Keywords\"] = res_df[\"Keywords\"].apply(lambda d: dict(sorted(d.items(), key=lambda item: item[1], reverse=True)))\n",
    "\n",
    "res_df"
   ]
  }
 ],
 "metadata": {
  "deepnote": {},
  "deepnote_app_layout": "article",
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "06d8ca2a193745bfbc3fe24a0db1c199",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
