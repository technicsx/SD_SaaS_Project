{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "a78848a5d0fd4e34a5897a47dc230dcc",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 1
    },
    "deepnote_cell_type": "text-cell-h1",
    "formattedRanges": [
     {
      "fromCodePoint": 0,
      "marks": {
       "bold": true
      },
      "toCodePoint": 17,
      "type": "marks"
     }
    ],
    "is_collapsed": false
   },
   "source": [
    "# ISW preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "6f16cd0deaeb4efd836f56e3018ad03d",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 7
    },
    "deepnote_cell_type": "text-cell-h3",
    "formattedRanges": [],
    "is_collapsed": false
   },
   "source": [
    "### Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "9b1434b44565402a970783f71fd427f1",
    "deepnote_app_coordinates": {
     "h": 17,
     "w": 12,
     "x": 0,
     "y": 13
    },
    "deepnote_app_is_output_hidden": true,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 7289,
    "execution_start": 1680019062987,
    "source_hash": "57d56a81"
   },
   "outputs": [],
   "source": [
    "# to be uncommented if dependencies are not installed\n",
    "\n",
    "# %pip install nltk num2words scikit-learn pandas numpy python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "7133bbff0c7644cdac4b3394e5327dd8",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 31
    },
    "deepnote_cell_type": "text-cell-h3",
    "formattedRanges": [],
    "is_collapsed": false
   },
   "source": [
    "### Import and download all dependecies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "8e138bae7b244bdea0d767dd645b62d8",
    "deepnote_app_coordinates": {
     "h": 25,
     "w": 12,
     "x": 0,
     "y": 37
    },
    "deepnote_app_is_output_hidden": true,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 2920,
    "execution_start": 1680019070283,
    "source_hash": "ec535853",
    "ExecuteTime": {
     "start_time": "2023-04-04T18:42:12.796580Z",
     "end_time": "2023-04-04T18:42:15.994819Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import pickle\n",
    "import re\n",
    "from num2words import num2words\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import string\n",
    "import os\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "3529039ed0544e6f984776537deef311",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 63
    },
    "deepnote_cell_type": "text-cell-h3",
    "formattedRanges": [],
    "is_collapsed": false
   },
   "source": [
    "### Define preprocessing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "86fdbb97c0564339842d296aedb07a7d",
    "deepnote_app_coordinates": {
     "h": 47,
     "w": 12,
     "x": 0,
     "y": 69
    },
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 5,
    "execution_start": 1680019073219,
    "source_hash": "bfe72237",
    "ExecuteTime": {
     "start_time": "2023-04-04T18:42:19.904177Z",
     "end_time": "2023-04-04T18:42:19.935430Z"
    }
   },
   "outputs": [],
   "source": [
    "# Functions\n",
    "def to_lower_case(text):\n",
    "  return \"\".join([i.lower() for i in text])\n",
    "\n",
    "stop_punctuation = string.punctuation\n",
    "def remove_punctuation(text):\n",
    "  return \"\".join([i for i in text if i not in stop_punctuation])\n",
    "\n",
    "def remove_long_dash(text):\n",
    "  return re.sub(r'â€”', ' ', text)\n",
    "\n",
    "def remove_urls(text):\n",
    "  return re.sub(r'http\\S+', '', text)\n",
    "\n",
    "def remove_one_letter_words(tokens):\n",
    "  return list(filter(lambda token: len(token) > 1, tokens))\n",
    "\n",
    "def tokenize_text(text):\n",
    "  return nltk.tokenize.word_tokenize(text)\n",
    "\n",
    "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "frequent_words = {'russian', 'force', 'forces', 'ukrainian', 'ukraine', 'oblast' 'ukraine', 'military', 'reported', 'effort', 'likely',\n",
    "                  'claimed', 'russia', 'area', 'operation', 'continued', 'city', 'general', 'near', 'attack',\n",
    "                  'official', 'staff', 'also', 'stated', 'source', 'oblast', 'pm', 'am'}\n",
    "month_names = {'january', 'february', 'march', 'april', 'may', 'june', 'july', 'august', 'september', 'october', 'november', 'december'}\n",
    "# avoid_stop_words = {\"not\", \"n't\", \"no\"}\n",
    "# stop_words = stop_words - avoid_stop_words\n",
    "stop_words = stop_words.union(frequent_words)\n",
    "stop_words = stop_words.union(month_names)\n",
    "\n",
    "def remove_stop_words(tokens):\n",
    "  return [i for i in tokens if i not in stop_words]\n",
    "\n",
    "def do_stemming(tokens):\n",
    "  ps = nltk.PorterStemmer()\n",
    "  return [ps.stem(word) for word in tokens]\n",
    "\n",
    "def do_lemmatization(tokens):\n",
    "  wn = nltk.WordNetLemmatizer()\n",
    "  return [wn.lemmatize(word) for word in tokens]\n",
    "\n",
    "def remove_numeric_words(text):\n",
    "  return re.sub(r'\\S*\\d+\\S*', '', text)\n",
    "\n",
    "def convert_nums_to_words(data):\n",
    "  tokens = data\n",
    "  new_text = []\n",
    "  for word in tokens:\n",
    "    if word.isdigit():\n",
    "      if int(word)<1000000000:\n",
    "        word = num2words(word)\n",
    "      else: \n",
    "        word = \"\"\n",
    "    new_text.extend(tokenize_text(re.sub(\"(-|,\\s?)|\\s+\", \" \", word)))\n",
    "  return new_text\n",
    "\n",
    "def do_preprocessing(data):\n",
    "  text_clean = data\n",
    "  text_clean = remove_urls(text_clean)\n",
    "  text_clean = remove_punctuation(text_clean)\n",
    "  text_clean = remove_long_dash(text_clean)\n",
    "  text_clean = to_lower_case(text_clean)\n",
    "  text_clean = remove_numeric_words(text_clean)\n",
    "  words = tokenize_text(text_clean)\n",
    "  words = remove_one_letter_words(words)\n",
    "  words = remove_stop_words(words)\n",
    "  lemmatized = do_lemmatization(words)\n",
    "  res = convert_nums_to_words(lemmatized)\n",
    "  return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "3adde034474b4e01b133850f0598de42",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 117
    },
    "deepnote_cell_type": "text-cell-h3",
    "formattedRanges": [],
    "is_collapsed": false
   },
   "source": [
    "### Zip opening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "9b0dd02d4a1d46c9910d60f25e24d79f",
    "deepnote_app_coordinates": {
     "h": 13,
     "w": 12,
     "x": 0,
     "y": 123
    },
    "deepnote_app_is_output_hidden": false,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 928,
    "execution_start": 1680019073228,
    "source_hash": "2e7a14d0",
    "ExecuteTime": {
     "start_time": "2023-04-04T19:04:26.662207Z",
     "end_time": "2023-04-04T19:04:26.883995Z"
    }
   },
   "outputs": [],
   "source": [
    "# use env config\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(\"../.path_env\")\n",
    "\n",
    "# specifying the input folder\n",
    "folder_name = \"../\" + os.getenv(\"ISW_SCRAPPING_FODLER\")\n",
    "\n",
    "df = pd.DataFrame(columns=[\"Name\", \"Date\", \"Text\"])\n",
    "\n",
    "df_list = []\n",
    "\n",
    "print(\"Reading folder contents\")\n",
    "for root, dirs, files in os.walk(folder_name):\n",
    "    for filename in files:\n",
    "        if filename.endswith('.txt'):\n",
    "            with open(os.path.join(root, filename), encoding='utf-8') as file:\n",
    "                name = filename.split('.')[0]\n",
    "                date = filename.replace(\"assessment-\", \"\").replace(\".txt\", \"\")\n",
    "                text = file.read()\n",
    "                row_df = pd.DataFrame({\"Name\": [name], \"Date\": [date], \"Text\": [text]})\n",
    "                df_list.append(row_df)\n",
    "df = pd.concat(df_list, ignore_index=True)\n",
    "print(\"Successfully read the input data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "17b1138cf7c9457aa477a30ccadacbf0",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 137
    },
    "deepnote_cell_type": "text-cell-h3",
    "formattedRanges": [],
    "is_collapsed": false
   },
   "source": [
    "### TF-IDF creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "09538d60d2d54f25bc752ee441583c16",
    "deepnote_app_coordinates": {
     "h": 44,
     "w": 12,
     "x": 0,
     "y": 143
    },
    "deepnote_cell_type": "code",
    "deepnote_table_loading": false,
    "deepnote_table_state": {
     "filters": [],
     "pageIndex": 0,
     "pageSize": 10,
     "sortBy": []
    },
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 95017,
    "execution_start": 1680019074182,
    "source_hash": "33982a8a",
    "ExecuteTime": {
     "start_time": "2023-04-04T19:04:34.298810Z",
     "end_time": "2023-04-04T19:05:18.174957Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Find tokens\")          \n",
    "df[\"Tokens\"] = df[\"Text\"].apply(lambda d: \" \".join(do_preprocessing(d)))\n",
    "\n",
    "# To be uncommented if you want to see the most common words\n",
    "#\n",
    "# print(\"Find most common words\")\n",
    "#\n",
    "# all_words = []\n",
    "# for tokens in df[\"Tokens\"]:\n",
    "#   for word in tokens.split(\" \"):\n",
    "#     all_words.append(word)\n",
    "# all_words = nltk.FreqDist(all_words)\n",
    "# print(\"Top 30 frequenty used words: \")\n",
    "# print(all_words.most_common(30))\n",
    "\n",
    "\n",
    "filenames = df[\"Name\"]\n",
    "dates = df[\"Date\"]\n",
    "\n",
    "print(\"Create vectors\")\n",
    "tfidf = TfidfVectorizer(smooth_idf=True,use_idf=True)\n",
    "vectors = tfidf.fit_transform(df[\"Tokens\"])\n",
    "\n",
    "# store content\n",
    "with open(\"results/tfidf.pkl\", \"wb\") as handle:\n",
    "  pickle.dump(tfidf, handle)\n",
    "\n",
    "\n",
    "feature_names = tfidf.get_feature_names_out()\n",
    "dense = vectors.todense()\n",
    "denselist = dense.tolist()\n",
    "df = pd.DataFrame(denselist, columns=feature_names)\n",
    "dictionaries = df.to_dict(orient='records')\n",
    "\n",
    "res = __builtins__.zip(filenames, dates, dictionaries)\n",
    "res_df = pd.DataFrame(res, columns=[\"Name\",\"Date\",\"Keywords\"])\n",
    "res_df[\"Keywords\"] = res_df[\"Keywords\"].apply(lambda d: {k: v for k, v in d.items() if v > 0})\n",
    "res_df[\"Keywords\"] = res_df[\"Keywords\"].apply(lambda d: dict(sorted(d.items(), key=lambda item: item[1], reverse=True)))\n",
    "\n",
    "res_df.to_csv('results/tfidf.csv', index=False)\n",
    "print(\"Successfully written to .csv\")"
   ]
  }
 ],
 "metadata": {
  "deepnote": {},
  "deepnote_app_layout": "article",
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "06d8ca2a193745bfbc3fe24a0db1c199",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
