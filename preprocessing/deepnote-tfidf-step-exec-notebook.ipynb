{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "a78848a5d0fd4e34a5897a47dc230dcc",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 1
    },
    "deepnote_cell_type": "text-cell-h1",
    "formattedRanges": [
     {
      "fromCodePoint": 0,
      "marks": {
       "bold": true
      },
      "toCodePoint": 17,
      "type": "marks"
     }
    ],
    "is_collapsed": false
   },
   "source": [
    "# ISW preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "6f16cd0deaeb4efd836f56e3018ad03d",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 7
    },
    "deepnote_cell_type": "text-cell-h3",
    "formattedRanges": [],
    "is_collapsed": false
   },
   "source": [
    "### Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cell_id": "9b1434b44565402a970783f71fd427f1",
    "deepnote_app_coordinates": {
     "h": 17,
     "w": 12,
     "x": 0,
     "y": 13
    },
    "deepnote_app_is_output_hidden": true,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 7289,
    "execution_start": 1680019062987,
    "source_hash": "57d56a81"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /shared-libs/python3.9/py/lib/python3.9/site-packages (3.7)\n",
      "Requirement already satisfied: num2words in /root/venv/lib/python3.9/site-packages (0.5.12)\n",
      "Requirement already satisfied: tqdm in /shared-libs/python3.9/py/lib/python3.9/site-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: click in /shared-libs/python3.9/py/lib/python3.9/site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: joblib in /shared-libs/python3.9/py/lib/python3.9/site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from nltk) (2022.9.13)\n",
      "Requirement already satisfied: docopt>=0.6.2 in /root/venv/lib/python3.9/site-packages (from num2words) (0.6.2)\n",
      "Requirement already satisfied: scikit-learn in /root/venv/lib/python3.9/site-packages (1.2.2)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from scikit-learn) (1.9.3)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from scikit-learn) (1.23.4)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from scikit-learn) (3.1.0)\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "# run pip upgrade if 'KeyboardInterrupt' error occurs\n",
    "# pip install --upgrade pip\n",
    "\n",
    "pip install nltk num2words\n",
    "pip install -U scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "7133bbff0c7644cdac4b3394e5327dd8",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 31
    },
    "deepnote_cell_type": "text-cell-h3",
    "formattedRanges": [],
    "is_collapsed": false
   },
   "source": [
    "### Import and download all dependecies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cell_id": "8e138bae7b244bdea0d767dd645b62d8",
    "deepnote_app_coordinates": {
     "h": 25,
     "w": 12,
     "x": 0,
     "y": 37
    },
    "deepnote_app_is_output_hidden": true,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 2920,
    "execution_start": 1680019070283,
    "source_hash": "ec535853"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "import pickle\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "from num2words import num2words\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "import pandas as pd\n",
    "import string\n",
    "from zipfile import ZipFile, ZipInfo\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "3529039ed0544e6f984776537deef311",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 63
    },
    "deepnote_cell_type": "text-cell-h3",
    "formattedRanges": [],
    "is_collapsed": false
   },
   "source": [
    "### Define preprocessing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cell_id": "86fdbb97c0564339842d296aedb07a7d",
    "deepnote_app_coordinates": {
     "h": 47,
     "w": 12,
     "x": 0,
     "y": 69
    },
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 5,
    "execution_start": 1680019073219,
    "source_hash": "bfe72237"
   },
   "outputs": [],
   "source": [
    "# Functions\n",
    "def to_lower_case(text):\n",
    "  return \"\".join([i.lower() for i in text])\n",
    "\n",
    "stop_punctuation = string.punctuation\n",
    "def remove_punctuation(text):\n",
    "  return \"\".join([i for i in text if i not in stop_punctuation])\n",
    "\n",
    "def remove_long_dash(text):\n",
    "  return re.sub(r'â€”', ' ', text)\n",
    "\n",
    "def remove_urls(text):\n",
    "  return re.sub(r'http\\S+', '', text)\n",
    "\n",
    "def remove_one_letter_words(tokens):\n",
    "  return list(filter(lambda token: len(token) > 1, tokens))\n",
    "\n",
    "def tokenize_text(text):\n",
    "  return nltk.tokenize.word_tokenize(text)\n",
    "\n",
    "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "avoid_stop_words = set([\"not\",\"n't\",\"no\"])\n",
    "stop_words = stop_words - avoid_stop_words\n",
    "\n",
    "def remove_stop_words(tokens):\n",
    "  return [i for i in tokens if i not in stop_words]\n",
    "\n",
    "def do_stemming(tokens):\n",
    "  ps = nltk.PorterStemmer()\n",
    "  return [ps.stem(word) for word in tokens]\n",
    "\n",
    "def do_lemmatization(tokens):\n",
    "  wn = nltk.WordNetLemmatizer()\n",
    "  return [wn.lemmatize(word) for word in tokens]\n",
    "\n",
    "def remove_numeric_words(text):\n",
    "  return re.sub(r'\\S*\\d+\\S*', '', text)\n",
    "\n",
    "def convert_nums_to_words(data):\n",
    "  tokens = data\n",
    "  new_text = []\n",
    "  for word in tokens:\n",
    "    if word.isdigit():\n",
    "      if int(word)<1000000000:\n",
    "        word = num2words(word)\n",
    "      else: \n",
    "        word = \"\"\n",
    "    new_text.extend(tokenize_text(re.sub(\"(-|,\\s?)|\\s+\", \" \", word)))\n",
    "  return new_text\n",
    "\n",
    "def do_preprocessing(data):\n",
    "  text_clean = data\n",
    "  text_clean = remove_urls(text_clean)\n",
    "  text_clean = remove_punctuation(text_clean)\n",
    "  text_clean = remove_long_dash(text_clean)\n",
    "  text_clean = to_lower_case(text_clean)\n",
    "  text_clean = remove_numeric_words(text_clean)\n",
    "  words = tokenize_text(text_clean)\n",
    "  words = remove_one_letter_words(words)\n",
    "  words = remove_stop_words(words)\n",
    "  lemmatized = do_lemmatization(words)\n",
    "  res = convert_nums_to_words(lemmatized)\n",
    "  return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "3adde034474b4e01b133850f0598de42",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 117
    },
    "deepnote_cell_type": "text-cell-h3",
    "formattedRanges": [],
    "is_collapsed": false
   },
   "source": [
    "### Zip opening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cell_id": "9b0dd02d4a1d46c9910d60f25e24d79f",
    "deepnote_app_coordinates": {
     "h": 13,
     "w": 12,
     "x": 0,
     "y": 123
    },
    "deepnote_app_is_output_hidden": false,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 928,
    "execution_start": 1680019073228,
    "source_hash": "2e7a14d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Openning zip in read mode\n"
     ]
    }
   ],
   "source": [
    "# specifying the zip file name\n",
    "file_name = \"/work/isw_scrapping_res.zip\"\n",
    "\n",
    "df = pd.DataFrame(columns = [\"Name\", \"Date\", \"Text\"])\n",
    "print(\"Openning zip in read mode\")\n",
    "# opening the zip file in READ mode\n",
    "with ZipFile(file_name, 'r') as zipfile:\n",
    "      for file in zipfile.infolist():\n",
    "        if not ZipInfo.is_dir(file):\n",
    "          filename = file.filename.rsplit('/', 1)[1].split('.')[0]\n",
    "          date = filename.replace(\"assessment-\", \"\")\n",
    "          text = zipfile.read(file.filename).decode('utf-8')\n",
    "          df = df.append({\"Name\": filename, \"Date\": date, \"Text\": text}, ignore_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "17b1138cf7c9457aa477a30ccadacbf0",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 137
    },
    "deepnote_cell_type": "text-cell-h3",
    "formattedRanges": [],
    "is_collapsed": false
   },
   "source": [
    "### TF-IDF creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cell_id": "09538d60d2d54f25bc752ee441583c16",
    "deepnote_app_coordinates": {
     "h": 44,
     "w": 12,
     "x": 0,
     "y": 143
    },
    "deepnote_cell_type": "code",
    "deepnote_table_loading": false,
    "deepnote_table_state": {
     "filters": [],
     "pageIndex": 0,
     "pageSize": 10,
     "sortBy": []
    },
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 95017,
    "execution_start": 1680019074182,
    "source_hash": "33982a8a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Find tokens\n"
     ]
    }
   ],
   "source": [
    "print(\"Find tokens\")          \n",
    "df[\"Tokens\"] = df[\"Text\"].apply(lambda d: \" \".join(do_preprocessing(d)))\n",
    "\n",
    "filenames = df[\"Name\"]\n",
    "dates = df[\"Date\"]\n",
    "\n",
    "print(\"Create vectors\")\n",
    "tfidf = TfidfVectorizer(smooth_idf=True,use_idf=True)\n",
    "vectors = tfidf.fit_transform(df[\"Tokens\"])\n",
    "\n",
    "# store content\n",
    "with open(\"/work/results/tfidf.pkl\", \"wb\") as handle:\n",
    "  pickle.dump(tfidf, handle)\n",
    "\n",
    "feature_names = tfidf.get_feature_names_out()\n",
    "dense = vectors.todense()\n",
    "denselist = dense.tolist()\n",
    "df = pd.DataFrame(denselist, columns=feature_names)\n",
    "dictionaries = df.to_dict(orient='records')\n",
    "\n",
    "print(\"Into result\")\n",
    "res = __builtins__.zip(filenames, dates, dictionaries)\n",
    "res_df = pd.DataFrame(res, columns=[\"Name\",\"Date\",\"Keywords\"])\n",
    "res_df[\"Keywords\"] = res_df[\"Keywords\"].apply(lambda d: {k: v for k, v in d.items() if v > 0})\n",
    "# sort by d.items\n",
    "res_df[\"Keywords\"] = res_df[\"Keywords\"].apply(lambda d: dict(sorted(d.items(), key=lambda item: item[1], reverse=True)))\n",
    "\n",
    "res_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "e242cea95d83442cb9f1ed85c15015ee",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 188
    },
    "deepnote_cell_type": "text-cell-h3",
    "formattedRanges": [],
    "is_collapsed": false
   },
   "source": [
    "### Forming zip with .csv output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cell_id": "aea029b8fb4c4edba10604e348a0b07e",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 194
    },
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": true,
    "execution_millis": 1068,
    "execution_start": 1680018717924,
    "source_hash": "779637eb"
   },
   "outputs": [],
   "source": [
    "filename = \"tfidf-result\"\n",
    "compression_options = dict(method='zip', archive_name=f'{filename}.csv')\n",
    "res_df.to_csv(f'/work/results/{filename}.zip', compression=compression_options, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": []
  }
 ],
 "metadata": {
  "deepnote": {},
  "deepnote_app_layout": "article",
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "06d8ca2a193745bfbc3fe24a0db1c199",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.2"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
